{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

üëãüáªüá≥ _Xin ch√†o!_ / _Hello!_ I'm **Anh Dao**, an undergraduate student in **Computer Science, Mathematics, and Applied Mathematics** at the Department of Computer Science and Engineering (**CSE**) of **Michigan State University (MSU)**, Honors College. I am an undergraduate research assistant in the **[Action Lab](https://www.egr.msu.edu/~yukong/)**, advised by Prof. [Yu Kong](https://www.egr.msu.edu/~yukong/).

During my undergraduate studies, I‚Äôve also had the privilege of working with Prof. [Hy Son (Truong-Son Hy)](http://people.cs.uchicago.edu/~hytruongson/), Prof. [Anh Nguyen](https://cgi.csc.liv.ac.uk/~anguyen/), and Prof. [Zijun Cui](https://zijunjkl.github.io/). I owe special thanks to [Yifan Li](https://jackyfl.github.io/) üôå, my first research mentor and a close collaborator who has shaped much of my early journey in computer vision and embodied AI.

My research spans **Vision-Language Models (VLMs)** and **Vision-Language-Action (VLA)** systems, with the goal of building agents that can understand physical environments, reason about humans and objects, and act intelligently in the real world.

To achieve this, my work tackles three fundamental challenges at the intersection of **computer vision, multimodal models, and embodied AI**:

1.  **Understanding üß≠:** How can we infuse VLMs with physical commonsense and rich 3D spatial understanding of their environments?
2.  **Reasoning üß†:** How can we enable agents to solve complex, long-horizon tasks that require memory, planning, and causal reasoning?
3.  **Efficiency ‚ö°Ô∏è:** How can we design efficient VLM/VLA models (e.g., via token reduction and optimized inference) that are fast and practical enough for real-world deployment?

I will be applying for **Ph.D. programs starting in Fall 2026** and am actively looking for opportunities to work with faculty on these and related problems in vision, robotics, and multimodal LLMs.

I am always open to collaboration‚Äîif you find overlapping interests, feel free to reach out! ü§ù
